{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Healthcare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be exploring reinforcement learning. We will begin by hypothesising the usefulness of reinforcement learning (RL) for healthcare; an application area in which it is seldom used. We will then go into RL, including the underlying statistical methodology of Markov decision processes (MDPs), and Q-learning. Finally, we hypothesise how RL could be used in healthcare, especially in the context of multimorbidity trajectory modelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro & Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) is a machine learning technique that has exploded in popularity in recent years, especially in areas with dynamic worlds such as real-time gaming and robotics. It is yet to gain popularity for use in healthcare, but there is some research emerging in this area, especially using offline RL. The primary goal of this project is to achieve a deep understanding of the methodology and mechanisms behind different types of RL (most of which are built on either a Q-learning or actor-critic based learning approach), as well as recent advances in deep RL. Through doing this, the goal is to then consider how RL can be used in the context of multimorbidity, by constructing an RL approach for modelling multimorbidity disease prediction over time ('multimorbidity trajectory modelling') using a multi-state model. Due to the methodological focus of this project, by studying the intricacies of different approaches in a thorough manner, the hope is that a deep understanding of RL will be gained. This will be useful in future research projects, whether that be applying RL to a specific healthcare problem, or even extending existing RL methodologies to advance the field. In summary, the objectives of this project (defined by C. Yau in https://github.com/cwcyau/hdruk-rl) are:\n",
    "- To develop a deep understanding of reinforcement learning, including the differences between varying approaches including Q-learning, actor-critic, and deep extensions.\n",
    "- To understand how to model multimorbidity trajectories using multi-state models.\n",
    "- To implement an RL approach to learn multi-state models for modelling multimorbidity trajectories. \n",
    "\n",
    "In this notebook, I will explain some of the key concepts, providing examples. Some of this content is inspired by DeepLizard's course on RL, which provides youtube videos on the key aspects of RL: https://youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov decision processes (MDPs) form the basis of reinforcement learning problems. Let's focus on the main components of an MDP in the context of RL:\n",
    "- <b>Agent</b>: This is the active decision maker in the problem. This could be anything: a person, an animal, a game character, a self-driving car, etc. This <b>agent</b> interacts with its <b>environment</b>.\n",
    "- <b>Environment</b>: The environment is the 'world'; where exciting and scary things might happen, and the <b>agent</b> wants to be prepared to handle these and act accordingly. \n",
    "- <b>State</b>: The <b>state</b>, $S_t$, is the current situation. This is composed of the environment's status at each timestep $t$, as well as the agent's position within the environment. With each timestep, the agent makes a choice about what to do (performs an action) given its environment, and will transition to a new state.\n",
    "- <b>Action</b>: The action is the choice that the agent makes at each timestep, given its environment and current state. This leads an agent from one state to another. The agent wants to make a good choice, and so each action has a corresponding reward - or set of possible rewards in a non-deterministic world. \n",
    "- <b>Reward</b>: A reward is recieved at each timestep, after the agent transitions from one state $S_t$ to another, $S_t+1$. The rewards are cumulative, so the aim of the agent is to maximise the cumulative rewards over time, not just the individual reward at each timestep. It 'looks ahead' to estimate this, but more weight is given to rewards that are in the near future rather than far away by discounting estimated rewards far away - this is called the _discounted reward_, and the discount is always less than 1). Rewards can also be negative.\n",
    "- <b>Transition Probability</b>: This is the probability that at time $t$, from state $S_t$, the agent will transition to state $S_t+1$ given it performs action $a$. This is 1 if the actions are deterministic (i.e. each action has only one outcome state). \n",
    "\n",
    "I'll now give a 'fun' health-related example that vaguely mimics a simplified version of real-life to illustrate this. At each time _t_ we have:\n",
    "- Agent: a person\n",
    "- Environment: the earth/their life \n",
    "- State: the set of diseases/comorbidities the person has \n",
    "- Actions: lifestyle choices or medications that affect the likelihood of the agent developing or losing comorbidities\n",
    "- Reward: the reward of taking an action and ending in a different state. For example, the reward of smoking might be very negative since it will increase the likelihood of developing cancer or high blood presure, and the reward of exercise may be positive but small since it results in no change to comorbidities. A simple reward function can be determined as the number of comorbidities present in a given state, for example.\n",
    "\n",
    "The ambition of the agent in this example is to battle diseases and remain as healthy as possible with minimal comorbidities, hence extending their lifespan; this will give the highest cumulative reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal representation and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should help with clarity to formalise the definitions above in mathematical notation. In this way, an MDP can be defined as follows:\n",
    "- $S$ is the set of states, $A$ is the set of actions, $R$ is the set of rewards.\n",
    "- We have $n$ timesteps $t$, where $t = 0,1,2,...,n-1$.\n",
    "- The states at each $t$ is $S_t$, where $S_t \\in S$.\n",
    "- The current state at each $t$ is $s$, where $s \\in S_t \\in S$.\n",
    "- At each $t$, the agent $ag$ performs one action $a$, where $A_t$ is the set of possible actions, and $a \\in A_t \\in A$.\n",
    "- The action taken from a given state can be defined as a state-action pair $(s, a)$.\n",
    "- The $ag$ recieves a reward $r$ after taking action $a$ from state $s$. This happens with a given transition probability, and $ag$ moves to the next state, $s'$.\n",
    "- The transition probability from $(s, a)$ to $s'$ can be defined as $p(s')$, as it is simply the probability of ending up in that state. This also means that $p(r = p(s'))$, which is chosen from a selection of possible next states, $S_{t+1}$.\n",
    "\n",
    "In general, $s' \\in S_{t+1} \\in S$ and $r \\in R_{t+1} \\in R$. In a deterministic world, $s = S_{t}$ and $r = R_{t+1}$, as following each $(s, a)$ results in only one possible outcome state $s'$ with one possible corresponding reward $r$.\n",
    "\n",
    "At each timestep, we reset the current state $s = s'$. The set of possible actions, $A_t$ is likely also updated.\n",
    "\n",
    "Hopefully as we continue with examples, this will all become clearer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can illustrate building the MDP described above in a simple code example. \n",
    "We begin with an individual with 0 comorbidities. We have 10 timesteps, and 3 comorbidities (obesity; heart disease; diabetes) that can develop or be cured. We have 3 possible actions that can be taken from each state, these are treatments (statins; stop smoking; exercise). In this example, the transition probabilities between each state are randomly generated. We print out each state and the transititon probabilities for each time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start state: {'diabetes': 0, 'heart_disease': 0}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.16368314025007727\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.23806016796153956\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.3018072623894381\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.296449429398945\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.41023082925854804\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.10013059321525963\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.2088168022729244\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.2808217752532679\n",
      "start state: {'diabetes': 1, 'heart_disease': 0}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.10223191197390931\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.37230390286487053\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.46007049061175975\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.06539369454946035\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.19986788440690556\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.27167485165446564\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.41381005787378206\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.11464720606484669\n",
      "start state: {'diabetes': 1, 'heart_disease': 1}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.21176603189244242\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.046059946092309796\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.23362839610180708\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.5085456259134407\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.345612221563223\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.10422250137022758\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.19370803292560337\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.3564572441409461\n",
      "start state: {'diabetes': 0, 'heart_disease': 1}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.33169439037757886\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.027186737364852255\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.04398191636337855\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.5971369558941905\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.20578902740459504\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.3097525505644477\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.11105862063091941\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.3733998014000378\n",
      "start state: {'diabetes': 0, 'heart_disease': 0}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.36510870877747303\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.05667910842852497\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.27956977475655154\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.29864240803745046\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.3113491210876049\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.12494472058624859\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.3151210679431917\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.24858509038295482\n",
      "start state: {'diabetes': 1, 'heart_disease': 0}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.12741679460192173\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.32411327980780463\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.49996584481073925\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.04850408077953444\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.11406354295010884\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.4027584113448706\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.1986748239725528\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.2845032217324677\n",
      "start state: {'diabetes': 1, 'heart_disease': 1}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.3605934499350036\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.21520857161563903\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.0946469123591994\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.3295510660901579\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.22979288814974316\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.21563954608751165\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.26583357135290353\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.2887339944098416\n",
      "start state: {'diabetes': 0, 'heart_disease': 1}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.3684722690786415\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.14840869961061257\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.46180551067244824\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.02131352063829774\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.08667644125719182\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.3266806721424052\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.5141266545155789\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.07251623208482401\n",
      "start state: {'diabetes': 0, 'heart_disease': 0}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.2585904113284647\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.34174775164363913\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.3099763953712588\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.08968544165663729\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.06289879705185063\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.3163880778432212\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.4063895722537724\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.21432355285115573\n",
      "start state: {'diabetes': 1, 'heart_disease': 0}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.09439544764987284\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.4760950188187867\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.10146773957735823\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.32804179395398225\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.30373436639233553\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.28215714720531027\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.09062407918973926\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.323484407212615\n",
      "start state: {'diabetes': 1, 'heart_disease': 1}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.17646872356198665\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.3128052735508588\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.3513425311364894\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.1593834717506653\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.36170344330340776\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.029230553094531923\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.2121320219214524\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.39693398168060795\n",
      "start state: {'diabetes': 0, 'heart_disease': 1}\n",
      "action taken: statin\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.37167021652081106\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.20715422111814963\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.24032223048960166\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.1808533318714376\n",
      "action taken: stop_smoking\n",
      "next state {'diabetes': 0, 'heart_disease': 0}\n",
      "transition probability 0.013200287123574711\n",
      "next state {'diabetes': 1, 'heart_disease': 0}\n",
      "transition probability 0.3081298899328007\n",
      "next state {'diabetes': 1, 'heart_disease': 1}\n",
      "transition probability 0.02943670207635331\n",
      "next state {'diabetes': 0, 'heart_disease': 1}\n",
      "transition probability 0.6492331208672713\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import random as rand\n",
    "#start off with dictionary of comorbidities being all 0s \n",
    "#then update this as we move through the mdp/policy \n",
    "\n",
    "comorbid_dict = {\n",
    "    'diabetes':0,\n",
    "    'heart_disease':0,\n",
    "}\n",
    "\n",
    "# finite number of states, encapsulating the possible comorbid dict values\n",
    "# states = [{'diabetes':0,'heart_disease':0,'obesity':0},\n",
    "#           {'diabetes':1,'heart_disease':0,'obesity':0},\n",
    "#           {'diabetes':1,'heart_disease':1,'obesity':0},\n",
    "#           {'diabetes':1,'heart_disease':0,'obesity':1},\n",
    "#           {'diabetes':1,'heart_disease':1,'obesity':1},\n",
    "#           {'diabetes':0,'heart_disease':1,'obesity':0},\n",
    "#           {'diabetes':0,'heart_disease':1,'obesity':1},\n",
    "#           {'diabetes':0,'heart_disease':0,'obesity':1}]\n",
    "\n",
    "# finite number of states, encapsulating the possible comorbid dict values\n",
    "states = [{'diabetes':0,'heart_disease':0},\n",
    "          {'diabetes':1,'heart_disease':0},\n",
    "          {'diabetes':1,'heart_disease':1},\n",
    "          {'diabetes':0,'heart_disease':1}]\n",
    "\n",
    "#actions are interventions such as drugs or lifestyle changes \n",
    "actions = ['statin', 'stop_smoking']\n",
    "\n",
    "#at each state, for each action, we transition to another state with a random probability \n",
    "\n",
    "max_t = 10\n",
    "t=0\n",
    "max_prob = 0.0\n",
    "\n",
    "df = DataFrame()\n",
    "\n",
    "while t < 10:\n",
    "    states_multi = [] \n",
    "    actions_multi = [] \n",
    "    for s in states:\n",
    "        print('start state:',s)\n",
    "        for a in actions:\n",
    "            \n",
    "            stateaction = s,a\n",
    "            states_multi.append(s)\n",
    "            actions_multi.append(a)\n",
    "                \n",
    "            print('action taken:', a )\n",
    "            i = 0\n",
    "            #generate random transition probabilities for each state, action pair \n",
    "            random_probs = np.random.random(len(states))\n",
    "            #make sure they add up to 1 \n",
    "            random_probs /= random_probs.sum()\n",
    "            \n",
    "            for s_1 in states:\n",
    "                \n",
    "                state_action = frozenset(sorted(s.items())),a\n",
    "                curr_prob = random_probs[i]\n",
    "                outcome_transition = frozenset(sorted(s_1.items())), curr_prob\n",
    "                transition = {state_action:outcome_transition}\n",
    "                print('next state', s_1)\n",
    "                print('transition probability', random_probs[i])\n",
    "                # this is what i need to put in the dataframe \n",
    "                i+=1\n",
    "            i=0\n",
    "        t+=1\n",
    "        \n",
    "state_actions_df = DataFrame(states_multi)\n",
    "state_actions_df['action'] = actions_multi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this forms the basis of the q-table, an entry for each possible state-action pair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabetes</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>statin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stop_smoking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>statin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>stop_smoking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>statin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stop_smoking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>statin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>stop_smoking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diabetes  heart_disease        action\n",
       "0         0              0        statin\n",
       "1         0              0  stop_smoking\n",
       "2         1              0        statin\n",
       "3         1              0  stop_smoking\n",
       "4         1              1        statin\n",
       "5         1              1  stop_smoking\n",
       "6         0              1        statin\n",
       "7         0              1  stop_smoking"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_actions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this MDP is built, we can solve this to find the optimal <b>policy</b>. A policy is defined as:\n",
    "- <b>Policy</b>: The policy is a function that tells an agent, from a given state, the probabilities of selecting each possible action. In a deterministic policy, it is a function that maps from a given state to only one action. The optimal policy is the policy with the highest expected cumulative reward (we will come to how this is calculated in a moment).\n",
    "\n",
    "Formally:\n",
    "- At timestep $t$, the policy can be defined as $Pol$, so the probability of selecting action $a$ from state $s$ can be written as $Pol(a|s)$.\n",
    "- Therefore, for each state $s$, $Pol$ is a probability distribution over $a \\in A_t$, which can be written as $Pol = a \\in A_t(s)$.\n",
    "\n",
    "What reinforcement learning does is find the optimal policy by exploring and exploiting the environment. It _learns_ this policy. In the upcoming sections, we will consider two of the main RL approaches: Q-learning and actor-critic learning, to illustrate this. \n",
    "\n",
    "But first, we must consider how to calculate the _expected rewards_ mentioned earlier. This basically means, how can we estimate how good it is to be in each state? Therefore estimating, how good is it to take a specific action from a given state? We can do this with help from the <b>value function</b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value functions are used to estimate the expective cumulative reward of a particular policy. This differs from the <b>reward function</b> in that this is the reward an agent actually recieves at each state, rather than the expected cumulative reward. \n",
    "\n",
    "Value functions are therefore defined with respect to policies, and there are two different types of value functions: \n",
    "1. <b>State value functions</b>: These estimate how 'good' any given state is at a particular time, when an agent is following a specific policy, denoted as $v_{Pol}$, which can be thought of as the 'value' of a state; this is different to the reward of the state in that it is the sum of the cumulative rewards up until this state, and the estimate of the rewards from this state (rather than just the reward of that single state). This can be written as $v_{Pol}(S_t)$. \n",
    "\n",
    "\n",
    "2. <b>Action value functions</b>: These estimate how 'good' a given action is, from a given state at a particular time, following a given policy - thought of as the 'value' of taking a given action. It is denoted as $q_{Pol}$ and can therefore be written as $q_{Pol}(S_t,A_t)$. This is referred to as the <b>Q-function</b>, where Q stands for _quality_ ðŸ˜Ž. \n",
    "\n",
    "This is where we get to the exciting part about reinforcement learning, actually solving the MDPs to generate optimal policies..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Policies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimal policy can simply be thought of as the policy that gives the greatest (or equally great) expected cumulative reward over all states in comparison to all alternative policies. Basically, it's the best path to take. \n",
    "\n",
    "Given this, it should come as no surprise that _optimal policies have corresponding optimal state value functions and action value functions_.\n",
    "\n",
    "Perhaps unsurprisingly, the optimal state value function is that with the highest expected value achievable by any policy for each state, for all states. This can be denoted as $v^{*}_{Pol}(s)$ for all $S$.\n",
    "\n",
    "Likewise, the optimal action value function (Q-function) is that with the largest expected value achievable by any policy for each possible state-action pair, for all states and all actions. This can be denoted as $q^{*}(s,a)$ for all $S$ and all $A(s)$.\n",
    "\n",
    "- <b>q*</b> must satisfy the _Bellman optimality equation_.\n",
    "    \n",
    "Sounds cool, right? It is. And it's important to remember as this is a key aspect of the Q-function. It states:\n",
    "\n",
    "For any state, action pair $(s,a)$ at timestep $t$, the Q-value is the expected reward for taking $a$ from $s$ (denoted as $r$), plus the _maximum_ expected discounted reward from _any_ possible next state, action pair $(s', a')$, by following the optimal policy from thereon. So then $(s')$ is the best possible next state, from which the best possible next action $(a')$, can be taken in the following timestep... etc. This forms the optimal Q-function, $q^{*}$.  In maths, we can say:\n",
    "\n",
    "$q^{*}(s,a) = E[r + max q^{*}(s', a')]$.\n",
    "    \n",
    "This might sound confusing at first but it's actually quite logical, and it should help when we explore how this is actually used. We use this equation to find $q^{*}$ (i.e. the best q-function). We then use RL to find the best $a$ from each $s$, following this policy, and hence determine the optmal policy. \n",
    "    \n",
    "This is where - finally - we get to the exciting stuff: how to do this using Q-learning! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will explore Q learning, which belongs to the family of RL approaches called value-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Q-Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning is a reinforcement learning method of solving an MDP, which learns the optimal policy. This means that the policy learned has the highest expected cumulative reward over all successive steps compared to alternative policies. \n",
    "\n",
    "Q-learning does this by learning the optimal Q-values for each state, action pair. This is $q*(s,a)$ and is calculated using the Q-function; which takes a state and and action and returns the expected discounted reward when following a given policy.\n",
    "\n",
    "Q-learning works using the concept of <b>Value Iteration</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, each state, action pair has a Q-value, $q$. \n",
    "Q-learning simply iteratively updates this value using the Bellman equation, by moving through the possible states and actions. It does this until the Q-function converges to the optimal Q-function, q*. This is value iteration. Here's a simple description of value iteration, in steps:\n",
    "\n",
    "In steps:\n",
    "- The agent begins with no idea about what future actions and states might exist - it knows only its current environment. It therefore also has no idea how good any action and its corresponding rewards will be. Because of this, all of the Q-values for each state, action pair can be initialised to 0. These are updated using value iteration, as the agent moves through its environment, and stored in a Q-table, which is a grid of values, with one entry for each action, state pair (ignoring time). \n",
    "- The agent makes a choice of any action from its start state. It then lands in another state, and recieves a reward. The Q-value is updated for the first state, action pair. It then moves from this state, into another, by choosing another action, and recieves another reward. This will continue until the timesteps limit is reached (_max steps_, which can be configured), or the 'game' is ended for another reason (i.e. in the 'game of life', the individual dies). \n",
    "- The agent will start again, and complete the same process, updating the Q-values as it moves through the world. In later simulations of this (the number of 'games played' can also be configured), the agent can have a look in the table, and see what a good next action to take might be.  \n",
    "    - BUT the danger of this is that it'll just choose the same action over and over again, and not explore enough alternative options (i.e. get stuck in a local maxima by <b>exploiting</b> the environment). \n",
    "    - HOWEVER if it just keeps <b>exploring</b> forever, and ignores the Q-values found already, it will never converge. \n",
    "- So, in order to decide which paths to take, Q-learning utilises the <b>explore/exploit tradeoff</b> - which can be configured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning explore/exploit strategy - Epsilon Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we configure the explore/exploit tradeoff? \n",
    "\n",
    "We can define an <b>exploration rate</b>, $\\epsilon$. This is the probability of choosing to explore rather than exploit. This is initially set to 1, to ensure that the agent starts off by exploring. \n",
    "\n",
    "$\\epsilon$ is decreased each time the game is started. So, as the agent learns more about the environment, the chance of exploiting increases and the agent becomes \"_greedy_\". Remember, exploring is randomly choosing an action, exploiting is choosing the action with the highest associated Q-value from the current state. However, when do we update this Q-value, if the state, action pair is revisited (e.g. at a later timestep) and the cumulative discounted reward therefore changes? \n",
    "\n",
    "This is done by using the <b>learning rate</b>, $Lr$: Between 0 and 1, this is basically how quickly an agent decides to update a Q-value for a given state, action pair. If set to 1, the old value will be completely replaced by the new value. 0 leaves the value as-is. Anything in between however, considers both the old and the new Q-values for that position - a higher $Lr$ results in more weight given to the new value. It is simply the weighted sum of the new _learned value_,  i.e. the cumulative discounted reward found for that state, action pair; and the old Q-value, following the calculation:\n",
    "\n",
    "$q'(s,a) = (1-Lr)q(s,a) + Lr(r + max q(s',a'))$\n",
    "\n",
    "Where $(r + max q(s',a'))$ is the new _learned value_, and $q(s,a)$ is the old Q-value.\n",
    "\n",
    "This $q'(s,a)$ then replaces the previous Q-value.\n",
    "\n",
    "This process continues, with multiple 'game plays', until the Q-function converges, and the optimal policy is found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on the simple MDP code we wrote earlier, we can implement this strategy to find the optimal q-function, q*, and optimal policy. \n",
    "\n",
    "For this, we will want to construct a q-table, to keep track of the q-values during value iteration, and set the parameters for the exploration and learning rate to ensure the explore/exploit tradeoff is considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up q-table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pandas import DataFrame\n",
    "# q table stores values for each possible state,action\n",
    "# store q value with each state,action as a dataframe\n",
    "\n",
    "q_table = state_actions_df\n",
    "q_table['q_value'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabetes</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>action</th>\n",
       "      <th>q_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>statin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stop_smoking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>statin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>stop_smoking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>statin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stop_smoking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>statin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>stop_smoking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diabetes  heart_disease        action  q_value\n",
       "0         0              0        statin        0\n",
       "1         0              0  stop_smoking        0\n",
       "2         1              0        statin        0\n",
       "3         1              0  stop_smoking        0\n",
       "4         1              1        statin        0\n",
       "5         1              1  stop_smoking        0\n",
       "6         0              1        statin        0\n",
       "7         0              1  stop_smoking        0"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need a function to calculate the reward\n",
    "#for now just return random int between 0 and 100 \n",
    "#but what we actually want to do is calculate the \n",
    "\n",
    "# def calc_reward(state,action):\n",
    "#the reward should be based on the outcome, not the existing state \n",
    "def calc_reward(start_state, action, resulting_state):\n",
    "    reward = 0 \n",
    "    #increase the reward if the outcome is not having the disease \n",
    "    #look at what we think will happen next\n",
    "    if(q_table.loc[resulting_state].diabetes == 0):\n",
    "        reward +=1\n",
    "    if(q_table.loc[resulting_state].heart_disease == 0):\n",
    "        reward +=1 \n",
    "    return reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_reward(start_state, action):\n",
    "    random_reward = np.random.randint(0,100)\n",
    "    return random_reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_state(start_state, states):\n",
    "    i = 0\n",
    "    trans_probs = DataFrame({\"state\":states})\n",
    "    trans_probs = trans_probs.reindex(columns = trans_probs.columns.tolist() + ['prob'])\n",
    "    while(i < len(states)):\n",
    "        #since we can move from any state to another, we assign transition probabilities to every state\n",
    "        prob = rand.random()\n",
    "        trans_probs.at[i,'prob'] = round(prob, 3)\n",
    "        i+=1\n",
    "    max_val = trans_probs.prob.max()\n",
    "    #then pick the most likely next state\n",
    "    likely_state = trans_probs.loc[trans_probs['prob'] == max_val, 'state'].iloc[0]\n",
    "    return likely_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes              0\n",
      "heart_disease         0\n",
      "action           statin\n",
      "q_value              65\n",
      "Name: 0, dtype: object\n",
      "exploring\n",
      "diabetes                    1\n",
      "heart_disease               0\n",
      "action           stop_smoking\n",
      "q_value                    29\n",
      "Name: 3, dtype: object\n",
      "exploring\n",
      "diabetes                    0\n",
      "heart_disease               1\n",
      "action           stop_smoking\n",
      "q_value                    53\n",
      "Name: 7, dtype: object\n",
      "exploring\n",
      "diabetes                    1\n",
      "heart_disease               1\n",
      "action           stop_smoking\n",
      "q_value                    53\n",
      "Name: 5, dtype: object\n",
      "exploring\n",
      "diabetes              1\n",
      "heart_disease         0\n",
      "action           statin\n",
      "q_value              48\n",
      "Name: 2, dtype: object\n",
      "exploring\n",
      "diabetes                    1\n",
      "heart_disease               1\n",
      "action           stop_smoking\n",
      "q_value                    53\n",
      "Name: 5, dtype: object\n",
      "exploring\n",
      "diabetes                    1\n",
      "heart_disease               1\n",
      "action           stop_smoking\n",
      "q_value                    53\n",
      "Name: 5, dtype: object\n",
      "exploring\n",
      "diabetes              1\n",
      "heart_disease         0\n",
      "action           statin\n",
      "q_value              70\n",
      "Name: 2, dtype: object\n",
      "exploring\n",
      "diabetes              0\n",
      "heart_disease         0\n",
      "action           statin\n",
      "q_value              91\n",
      "Name: 0, dtype: object\n",
      "exploiting\n",
      "diabetes              0\n",
      "heart_disease         0\n",
      "action           statin\n",
      "q_value              91\n",
      "Name: 0, dtype: object\n",
      "exploring\n",
      "diabetes              0\n",
      "heart_disease         0\n",
      "action           statin\n",
      "q_value              85\n",
      "Name: 0, dtype: object\n",
      "exploring\n",
      "diabetes                    1\n",
      "heart_disease               1\n",
      "action           stop_smoking\n",
      "q_value                    53\n",
      "Name: 5, dtype: object\n",
      "exploring\n",
      "diabetes                    1\n",
      "heart_disease               0\n",
      "action           stop_smoking\n",
      "q_value                    28\n",
      "Name: 3, dtype: object\n",
      "exploiting\n",
      "diabetes              1\n",
      "heart_disease         1\n",
      "action           statin\n",
      "q_value              57\n",
      "Name: 4, dtype: object\n",
      "exploiting\n",
      "diabetes              0\n",
      "heart_disease         1\n",
      "action           statin\n",
      "q_value              49\n",
      "Name: 6, dtype: object\n",
      "exploiting\n",
      "best policy:  [0, 4, 0, 4, 2, 3, 4, 7, 1, 0, 4, 0, 3, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "i = 0 \n",
    "#set start state by index (we will start at 0)\n",
    "start_state = 0\n",
    "max_steps = 15\n",
    "explore = True \n",
    "policy = []\n",
    "#set learning rate \n",
    "#set exploration rate\n",
    "exploration_rate = 1\n",
    "learning_rate = 0.8\n",
    "disc_factor = 0.001 \n",
    "num_states = len(q_table)\n",
    "    \n",
    "while i < max_steps:\n",
    "    #make choice of any action - needs to be the action in the state we are in \n",
    "    #think this is not right, we need to have it separately somewhere to pick an action independently of the state\n",
    "    #at the moment all of the states are attached to an action when you enter it so the policy is kinda predetermined which is not what we want \n",
    "    action = q_table.at[start_state,'action']\n",
    "\n",
    "    #we don't yet know what the next state is, so we have to guess based on probability transitions to each next state\n",
    "    #pick the one with the highest probability \n",
    "    states = q_table.index.to_list()\n",
    "    likely_next_state = find_next_state(start_state, states)\n",
    "    \n",
    "    new_reward_at_state = calc_reward(start_state, action, likely_next_state)\n",
    "    curr_reward_at_state = q_table.at[start_state,'q_value']\n",
    "    \n",
    "    policy.append(start_state)\n",
    "    \n",
    "\n",
    "    max_reward_next_state = calc_max_reward(likely_next_state, action)\n",
    "    \n",
    "    #update the q value with a weighted sum of new value and prev value (TD)\n",
    "    temp_diff = new_reward_at_state + (disc_factor* max_reward_next_state) - curr_reward_at_state  \n",
    "    new_q_val = curr_reward_at_state + learning_rate * temp_diff\n",
    "#     using eqn from here: https://www.youtube.com/watch?v=mcfivkwM4p8\n",
    "#     new_q_val = ((1-learning_rate)*curr_reward_at_state) + (learning_rate*temp_diff)\n",
    "    q_table.at[start_state,'q_value'] = new_q_val\n",
    "    curr_reward_at_state = new_q_val    \n",
    "    #generate random number between 0 and 1 \n",
    "    #if less than explore rate then we choose to explore (pick random next state) not exploit (pick best next state)\n",
    "    rand_n = random.random()\n",
    "    if(rand_n<exploration_rate):\n",
    "        print('exploring')\n",
    "        #we want to have a probability for transitioning to each possible next state \n",
    "        next_state = find_next_state(start_state, states)\n",
    "        while(start_state == next_state):\n",
    "            next_state = find_next_state(start_state,states)\n",
    "        start_state = next_state\n",
    "        explore = True\n",
    "    else:\n",
    "        print('exploiting')\n",
    "        max_r_state = q_table[q_table.q_value == q_table.q_value.max()]\n",
    "        #get row at this value \n",
    "        max_r_state_index = max_r_state.index[0]\n",
    "        #this means we can stay in the same state but i think this is fine if this stays the best choice?\n",
    "        next_state = max_r_state_index\n",
    "        start_state = next_state\n",
    "        explore = False\n",
    "    i+=1\n",
    "    \n",
    "    if(exploration_rate > 0.05):\n",
    "        exploration_rate -= 0.05\n",
    "    if(learning_rate > 0.05):\n",
    "        learning_rate -=0.05\n",
    "    \n",
    "    \n",
    "        \n",
    "print(\"best policy: \", policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabetes</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>action</th>\n",
       "      <th>q_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>statin</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stop_smoking</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>statin</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>stop_smoking</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>statin</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stop_smoking</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>statin</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>stop_smoking</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diabetes  heart_disease        action  q_value\n",
       "0         0              0        statin       30\n",
       "1         0              0  stop_smoking       50\n",
       "2         1              0        statin       40\n",
       "3         1              0  stop_smoking       29\n",
       "4         1              1        statin       53\n",
       "5         1              1  stop_smoking       52\n",
       "6         0              1        statin       49\n",
       "7         0              1  stop_smoking       28"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that has hopefully made clear the basics of Q-learning and how a simple example might be implemented, we will shift focus to another key reinforcement learning paradigm: policy-gradient methods. These are appealing since rather than generating deterministic policy (like Q-learning does, using a value-based approach), they return stochastic policies, which are distributions over actions - this makes more sense for a lot of RL problems. However, there are issues with the basic policy-gradient approach, which we will explain - this has lead to the development of actor-critic learning techniques. We will go into these in detail, in a similar way as we did for Q-learning, to ensure understanding of this approach in the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
